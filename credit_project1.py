# -*- coding: utf-8 -*-
"""credit_project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aBgDMUb_5pH1pl5C13lXVbqado6pfcpH
"""

#!/usr/bin/env python3
# credit_project1.py â€” Clean, GitHub-ready version

"""
Credit Risk Prediction (cs-training.csv)
- LightGBM + SHAP + LIME
- Produces credit_output/ and credit_output.zip with artifacts

Before running:
pip install lightgbm xgboost shap lime imbalanced-learn joblib matplotlib pandas numpy scikit-learn
Place cs-training.csv beside this script (or supply --datafile).
"""

import os
import shutil
import warnings
import joblib
import argparse

warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix

from imblearn.over_sampling import SMOTE
from lime.lime_tabular import LimeTabularExplainer


def run(datafile="cs-training.csv", outdir="credit_output", random_state=42, test_size=0.2):
    os.makedirs(outdir, exist_ok=True)

    if not os.path.exists(datafile):
        raise FileNotFoundError(f"{datafile} not found. Download it from Kaggle and place next to this script.")

    print("Loading dataset:", datafile)
    df = pd.read_csv(datafile, low_memory=False)
    TARGET = "SeriousDlqin2yrs"
    if TARGET not in df.columns:
        raise ValueError(f"Target column '{TARGET}' not found in dataset.")

    df[TARGET] = df[TARGET].astype(int)
    print("Target distribution:\n", df[TARGET].value_counts())

    # Basic cleaning
    drop_manual = [c for c in ["Unnamed: 0", "Id", "id", "loan_id", "member_id"] if c in df.columns]
    if drop_manual:
        df = df.drop(columns=drop_manual, errors="ignore")

    # Drop constants and very high missingness
    nunique = df.nunique(dropna=False)
    df = df.drop(columns=nunique[nunique <= 1].index, errors="ignore")
    df = df.drop(columns=df.columns[df.isna().mean() > 0.9], errors="ignore")

    # Split
    X = df.drop(columns=[TARGET])
    y = df[TARGET].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)
    print("Train/test shapes:", X_train.shape, X_test.shape)

    # Preprocessing: numeric and categorical
    numeric_cols = X.select_dtypes(include=["number"]).columns.tolist()
    categorical_cols = X.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

    num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
    cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="constant", fill_value="__MISSING__")),
                         ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))])

    preprocessor = ColumnTransformer(
        [("num", num_pipe, numeric_cols),
         ("cat", cat_pipe, categorical_cols)],
        remainder="drop"
    )

    print("Fitting preprocessor...")
    X_train_p = preprocessor.fit_transform(X_train)
    X_test_p = preprocessor.transform(X_test)

    # Build feature names robustly
    ohe_names = []
    if categorical_cols:
        try:
            ohe = preprocessor.named_transformers_["cat"].named_steps["onehot"]
            ohe_names = ohe.get_feature_names_out(categorical_cols).tolist()
        except Exception:
            # fallback: create generic names for OHE columns
            n_ohe_cols = X_train_p.shape[1] - len(numeric_cols)
            ohe_names = [f"cat_{i}" for i in range(n_ohe_cols)]
    feature_names = numeric_cols + ohe_names

    # SMOTE balancing
    print("Before SMOTE:", np.bincount(y_train))
    sm = SMOTE(random_state=random_state)
    X_train_bal, y_train_bal = sm.fit_resample(X_train_p, y_train)
    print("After SMOTE:", np.bincount(y_train_bal))

    # LightGBM training (random search)
    clf = lgb.LGBMClassifier(random_state=random_state, n_jobs=-1)
    param_dist = {
        "num_leaves": [31, 50, 80],
        "learning_rate": [0.01, 0.05, 0.1],
        "n_estimators": [100, 300, 500],
        "min_child_samples": [20, 50],
        "subsample": [0.6, 0.8, 1.0],
    }
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)
    search = RandomizedSearchCV(clf, param_dist, n_iter=12, scoring="roc_auc", cv=cv, n_jobs=-1, random_state=random_state, verbose=0)
    print("Running randomized search (LightGBM)...")
    search.fit(X_train_bal, y_train_bal)
    best_lgb = search.best_estimator_
    print("Best params:", search.best_params_)

    # Evaluate
    y_proba = best_lgb.predict_proba(X_test_p)[:, 1]
    y_pred = (y_proba >= 0.5).astype(int)

    auc = roc_auc_score(y_test, y_proba)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)
    cm = confusion_matrix(y_test, y_pred)

    with open(os.path.join(outdir, "metrics.txt"), "w") as f:
        f.write(f"AUC: {auc:.4f}\n")
        f.write(f"Precision: {prec:.4f}\n")
        f.write(f"Recall: {rec:.4f}\n")
        f.write(f"F1: {f:.4f}\n")
        f.write("Confusion Matrix:\n")
        f.write(np.array2string(cm) + "\n")

    print("Saved metrics.txt")

    # Save model + preprocessor
    joblib.dump(best_lgb, os.path.join(outdir, "best_lgb.pkl"))
    joblib.dump(preprocessor, os.path.join(outdir, "preprocessor.pkl"))

    # SHAP global explanation
    try:
        explainer = shap.TreeExplainer(best_lgb)
        # ensure columns align
        X_test_df = pd.DataFrame(X_test_p, columns=feature_names)
        shap_values = explainer.shap_values(X_test_df)
        shap_vals_use = shap_values[1] if isinstance(shap_values, list) else shap_values

        plt.figure(figsize=(10, 6))
        shap.summary_plot(shap_vals_use, X_test_df, show=False)
        plt.tight_layout()
        plt.savefig(os.path.join(outdir, "shap_summary.png"), dpi=150)
        plt.close()

        shap_summary = pd.DataFrame({
            "feature": feature_names,
            "mean_abs_shap": np.abs(shap_vals_use).mean(axis=0)
        }).sort_values("mean_abs_shap", ascending=False)
        shap_summary.to_csv(os.path.join(outdir, "shap_feature_importance.csv"), index=False)
        print("Saved SHAP artifacts.")
    except Exception as e:
        print("SHAP failed:", e)

    # Simple age bias check (if present)
    if "age" in X.columns:
        tmp = df.copy()
        tmp["age_bucket"] = pd.cut(tmp["age"], bins=[0, 30, 45, 60, 100], labels=["<=30", "31-45", "46-60", ">60"])
        rates = tmp.groupby("age_bucket")[TARGET].mean()
        with open(os.path.join(outdir, "bias_check.txt"), "w") as f:
            f.write(rates.to_string())
        print("Saved bias_check.txt")

    # Zip outputs
    if os.path.exists("credit_output.zip"):
        os.remove("credit_output.zip")
    shutil.make_archive("credit_output", 'zip', outdir)
    print("Created credit_output.zip in working directory.")

    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Credit risk pipeline (cs-training.csv)")
    parser.add_argument("--datafile", default="cs-training.csv", help="Path to cs-training.csv")
    parser.add_argument("--outdir", default="credit_output", help="Output folder")
    args = parser.parse_args()

    run(datafile=args.datafile, outdir=args.outdir)